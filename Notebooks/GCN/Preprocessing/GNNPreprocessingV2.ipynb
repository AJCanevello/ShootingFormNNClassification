{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "doJ9W0kcrorQ"
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip uninstall -y numpy opencv-python opencv-python-headless mediapipe protobuf > /dev/null 2>&1\n",
    "!pip install --upgrade pip > /dev/null 2>&1\n",
    "!pip install numpy==1.26.4 opencv-python-headless==4.8.0.76 mediapipe==0.10.10 protobuf==3.20.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZymhgKspP0S8"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Import libraries\n",
    "import os\n",
    "import cv2\n",
    "import json\n",
    "import numpy as np\n",
    "from google.colab import drive\n",
    "import mediapipe as mp\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "\n",
    "# Initialize MediaPipe\n",
    "mp_pose = mp.solutions.pose\n",
    "mp_hands = mp.solutions.hands\n",
    "\n",
    "# Define video paths\n",
    "video_paths = [\n",
    "        \"/content/drive/MyDrive/E-RAU(DB)/MA680/data/Shooting/Extra Data/Experienced/Video/Tr1_NIE.MOV\",\n",
    "        \"/content/drive/MyDrive/E-RAU(DB)/MA680/data/Shooting/Extra Data/Experienced/Video/JoeSupportSide.MOV\",\n",
    "        \"/content/drive/MyDrive/E-RAU(DB)/MA680/data/Shooting/Extra Data/Experienced/Video/JoeStrongSide.MOV\",\n",
    "        \"/content/drive/MyDrive/E-RAU(DB)/MA680/data/Shooting/Extra Data/Experienced/Video/Tr2_NIE.MOV\",\n",
    "        \"/content/drive/MyDrive/E-RAU(DB)/MA680/data/Shooting/Extra Data/Novice/Videos/Te1_1.MOV\",\n",
    "        \"/content/drive/MyDrive/E-RAU(DB)/MA680/data/Shooting/Extra Data/Novice/Videos/Te2_2.MOV\",\n",
    "        \"/content/drive/MyDrive/E-RAU(DB)/MA680/data/Shooting/Extra Data/Novice/Videos/Te2_1.MOV\",\n",
    "        \"/content/drive/MyDrive/E-RAU(DB)/MA680/data/Shooting/Extra Data/Novice/Videos/Te2_2.MOV\",\n",
    "        \"/content/drive/MyDrive/E-RAU(DB)/MA680/data/Shooting/Extra Data/Novice/Videos/Te3_1.MOV\",\n",
    "        \"/content/drive/MyDrive/E-RAU(DB)/MA680/data/Shooting/Extra Data/Novice/Videos/Te3_2.MOV\",\n",
    "        \"/content/drive/MyDrive/E-RAU(DB)/MA680/data/Shooting/Extra Data/Novice/Videos/Te4_1.MOV\",\n",
    "        \"/content/drive/MyDrive/E-RAU(DB)/MA680/data/Shooting/Extra Data/Novice/Videos/Te4_2.MOV\",\n",
    "        \"/content/drive/MyDrive/E-RAU(DB)/MA680/data/Shooting/Extra Data/Novice/Videos/Te5_1.MOV\",\n",
    "        \"/content/drive/MyDrive/E-RAU(DB)/MA680/data/Shooting/Extra Data/Novice/Videos/Te5_2.MOV\",\n",
    "        \"/content/drive/MyDrive/E-RAU(DB)/MA680/data/Shooting/Extra Data/Novice/Videos/Te6_1.MOV\",\n",
    "        \"/content/drive/MyDrive/E-RAU(DB)/MA680/data/Shooting/Extra Data/Novice/Videos/Te6_2.MOV\",\n",
    "        \"/content/drive/MyDrive/E-RAU(DB)/MA680/data/Shooting/Extra Data/Novice/Videos/Te7_1.MOV\",\n",
    "        \"/content/drive/MyDrive/E-RAU(DB)/MA680/data/Shooting/Extra Data/Novice/Videos/Te7_2.MOV\",\n",
    "        \"/content/drive/MyDrive/E-RAU(DB)/MA680/data/Shooting/Extra Data/Novice/Videos/Te8_1.MOV\",\n",
    "        \"/content/drive/MyDrive/E-RAU(DB)/MA680/data/Shooting/Extra Data/Novice/Videos/Te8_2.MOV\",\n",
    "        \"/content/drive/MyDrive/E-RAU(DB)/MA680/data/Shooting/Extra Data/Novice/Videos/Te9_1.MOV\",\n",
    "        \"/content/drive/MyDrive/E-RAU(DB)/MA680/data/Shooting/Extra Data/Novice/Videos/Te9_2.MOV\",\n",
    "        \"/content/drive/MyDrive/E-RAU(DB)/MA680/data/Shooting/Extra Data/Novice/Videos/Te10_1.MOV\",\n",
    "        \"/content/drive/MyDrive/E-RAU(DB)/MA680/data/Shooting/Extra Data/Novice/Videos/Te10_2.MOV\",\n",
    "        \"/content/drive/MyDrive/E-RAU(DB)/MA680/data/Shooting/Extra Data/Novice/Videos/Te11_1.MOV\",\n",
    "        \"/content/drive/MyDrive/E-RAU(DB)/MA680/data/Shooting/Extra Data/Novice/Videos/Te11_2.MOV\",\n",
    "        \"/content/drive/MyDrive/E-RAU(DB)/MA680/data/Shooting/Extra Data/Experienced/Video/Tr3_Anticipating.MOV\",\n",
    "        \"/content/drive/MyDrive/E-RAU(DB)/MA680/data/Shooting/Extra Data/Experienced/Video/Tr4_TooLittleTriggerFinger.MOV\",\n",
    "        \"/content/drive/MyDrive/E-RAU(DB)/MA680/data/Shooting/Extra Data/Experienced/Video/Tr5_TooMuchTriggerFinger.MOV\",\n",
    "        \"/content/drive/MyDrive/E-RAU(DB)/MA680/data/Shooting/Extra Data/Experienced/Video/Tr6_OvergrippingWithPrimaryHand.MOV\",\n",
    "        \"/content/drive/MyDrive/E-RAU(DB)/MA680/data/Shooting/Extra Data/Experienced/Video/Tr7_OvergrippingWithSecondaryHand.MOV\",\n",
    "        \"/content/drive/MyDrive/E-RAU(DB)/MA680/data/Shooting/Extra Data/Experienced/Video/Tr8_BreakingWristUp.MOV\",\n",
    "        \"/content/drive/MyDrive/E-RAU(DB)/MA680/data/Shooting/Extra Data/Experienced/Video/Tr9_CheckingTargetOverSightsBetweenShots.MOV\",\n",
    "        \"/content/drive/MyDrive/E-RAU(DB)/MA680/data/Shooting/Extra Data/Experienced/Video/Tr10_JerkingTrigger.MOV\",\n",
    "        \"/content/drive/MyDrive/E-RAU(DB)/MA680/data/Shooting/Extra Data/Experienced/Video/Tr11_LimpWristing.MOV\",\n",
    "        \"/content/drive/MyDrive/E-RAU(DB)/MA680/data/Shooting/Extra Data/Experienced/Video/Tr13_SightsIncorrectlyAlligned.MOV\",\n",
    "        \"/content/drive/MyDrive/E-RAU(DB)/MA680/data/Shooting/Extra Data/Experienced/Video/Tr12_GrippingTooLow.MOV\",\n",
    "        \"/content/drive/MyDrive/E-RAU(DB)/MA680/data/Shooting/Extra Data/Experienced/Video/Tr14_IncorrectHandPlacement.MOV\",\n",
    "]\n",
    "\n",
    "# Output directories\n",
    "graph_output_dir = \"/content/drive/MyDrive/E-RAU(DB)/MA680/data/Shooting/Processed_Graph_Data\"\n",
    "frames_output_dir = \"/content/drive/MyDrive/E-RAU(DB)/MA680/data/Shooting/Processed_Frames\"\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(graph_output_dir, exist_ok=True)\n",
    "os.makedirs(frames_output_dir, exist_ok=True)\n",
    "\n",
    "# GRAPH STRUCTURE DEFINITION\n",
    "\n",
    "def create_graph_structure():\n",
    "    \"\"\"Define the anatomical connections between keypoints\"\"\"\n",
    "    pose_edges = [\n",
    "        (0,1),(1,2),(2,3),(3,7),(0,4),(4,5),\n",
    "        (5,6),(6,8),(9,10),(11,12),(11,13),\n",
    "        (13,15),(15,17),(15,19),(15,21),(17,19),\n",
    "        (12,14),(14,16),(16,18),(16,20),(16,22),\n",
    "        (18,20),(11,23),(12,24),(23,24),(23,25),\n",
    "        (24,26),(25,27),(26,28),(27,29),(28,30),\n",
    "        (29,31),(30,32)\n",
    "    ]\n",
    "\n",
    "    hand_edges = [\n",
    "        (0,1),(1,2),(2,3),(3,4),(0,5),(5,6),\n",
    "        (6,7),(7,8),(0,9),(9,10),(10,11),(11,12),\n",
    "        (0,13),(13,14),(14,15),(15,16),(0,17),\n",
    "        (17,18),(18,19),(19,20)\n",
    "    ]\n",
    "\n",
    "    return {\n",
    "        'pose_edges': pose_edges,\n",
    "        'hand_edges': hand_edges,\n",
    "        'offsets': {\n",
    "            'pose': 0,\n",
    "            'left_hand': 33,\n",
    "            'right_hand': 54\n",
    "        }\n",
    "    }\n",
    "\n",
    "# KEYPOINT EXTRACTION\n",
    "\n",
    "def extract_keypoints(frame, pose, hands):\n",
    "    \"\"\"Extract pose and hand keypoints from a frame\"\"\"\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    pose_results = pose.process(frame_rgb)\n",
    "    hand_results = hands.process(frame_rgb)\n",
    "\n",
    "    keypoints = []\n",
    "    visibility = []\n",
    "\n",
    "    # Pose (33 points)\n",
    "    if pose_results.pose_landmarks:\n",
    "        for lm in pose_results.pose_landmarks.landmark:\n",
    "            keypoints.append([lm.x, lm.y, lm.z])\n",
    "            visibility.append(lm.visibility)\n",
    "    else:\n",
    "        keypoints.extend([[0,0,0] for _ in range(33)])\n",
    "        visibility.extend([0] * 33)\n",
    "\n",
    "    # Hands (21 points each)\n",
    "    hand_status = {'left': False, 'right': False}\n",
    "\n",
    "    if hand_results.multi_hand_landmarks:\n",
    "        for hand_landmarks, handedness in zip(hand_results.multi_hand_landmarks,\n",
    "                                            hand_results.multi_handedness):\n",
    "            hand_type = handedness.classification[0].label.lower()\n",
    "            hand_status[hand_type] = True\n",
    "\n",
    "            for lm in hand_landmarks.landmark:\n",
    "                keypoints.append([lm.x, lm.y, lm.z])\n",
    "                visibility.append(1.0)\n",
    "\n",
    "    # Fill missing hands\n",
    "    for hand_type in ['left', 'right']:\n",
    "        if not hand_status[hand_type]:\n",
    "            keypoints.extend([[0,0,0] for _ in range(21)])\n",
    "            visibility.extend([0] * 21)\n",
    "\n",
    "    return np.array(keypoints), np.array(visibility)\n",
    "\n",
    "# VIDEO PROCESSING FUNCTION\n",
    "\n",
    "def process_video(video_path, output_dir, frames_dir=None, interval=10):\n",
    "    \"\"\"Process video into graph data while optionally saving original frames\"\"\"\n",
    "    if not os.path.exists(video_path):\n",
    "        print(f\"Video not found: {video_path}\")\n",
    "        return\n",
    "\n",
    "    video_name = os.path.splitext(os.path.basename(video_path))[0]\n",
    "    graph_video_dir = os.path.join(output_dir, video_name)\n",
    "    os.makedirs(graph_video_dir, exist_ok=True)\n",
    "\n",
    "    # Create frames directory if specified\n",
    "    if frames_dir:\n",
    "        frames_video_dir = os.path.join(frames_dir, video_name)\n",
    "        os.makedirs(frames_video_dir, exist_ok=True)\n",
    "\n",
    "    graph_config = create_graph_structure()\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    frame_count = 0\n",
    "    processed_count = 0\n",
    "\n",
    "    with mp_pose.Pose(\n",
    "        static_image_mode=False,\n",
    "        model_complexity=2,\n",
    "        enable_segmentation=False,\n",
    "        min_detection_confidence=0.5\n",
    "    ) as pose, mp_hands.Hands(\n",
    "        static_image_mode=False,\n",
    "        max_num_hands=2,\n",
    "        min_detection_confidence=0.5,\n",
    "        min_tracking_confidence=0.5\n",
    "    ) as hands:\n",
    "\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            if frame_count % interval == 0:\n",
    "                # Save original frame if requested\n",
    "                frame_path = None\n",
    "                if frames_dir:\n",
    "                    frame_path = os.path.join(frames_video_dir, f\"frame_{frame_count:04d}.jpg\")\n",
    "                    cv2.imwrite(frame_path, frame)\n",
    "\n",
    "                # Extract keypoints\n",
    "                keypoints, visibility = extract_keypoints(frame, pose, hands)\n",
    "\n",
    "                # Build graph structure\n",
    "                edges = graph_config['pose_edges'].copy()\n",
    "                for hand_type in ['left_hand', 'right_hand']:\n",
    "                    offset = graph_config['offsets'][hand_type]\n",
    "                    edges.extend([(offset+a, offset+b) for (a,b) in graph_config['hand_edges']])\n",
    "\n",
    "                # Prepare graph data\n",
    "                graph_data = {\n",
    "                    'nodes': keypoints.tolist(),\n",
    "                    'edges': edges,\n",
    "                    'visibility': visibility.tolist(),\n",
    "                    'frame': frame_count,\n",
    "                    'timestamp': frame_count / fps,\n",
    "                    'video': video_name\n",
    "                }\n",
    "\n",
    "                # Add frame reference if saved\n",
    "                if frame_path:\n",
    "                    graph_data['original_frame'] = frame_path\n",
    "\n",
    "                # Save graph data\n",
    "                graph_path = os.path.join(graph_video_dir, f\"frame_{frame_count:04d}.json\")\n",
    "                with open(graph_path, 'w') as f:\n",
    "                    json.dump(graph_data, f, indent=2)\n",
    "\n",
    "                processed_count += 1\n",
    "                print(f\"Processed frame {frame_count}\", end='\\r')\n",
    "\n",
    "            frame_count += 1\n",
    "\n",
    "        cap.release()\n",
    "        print(f\"\\nFinished processing {video_name}: {processed_count} frames extracted\")\n",
    "\n",
    "# MAIN PROCESSING LOOP\n",
    "\n",
    "\n",
    "for video_path in video_paths:\n",
    "    print(f\"\\nProcessing {os.path.basename(video_path)}...\")\n",
    "    process_video(\n",
    "        video_path=video_path,\n",
    "        output_dir=graph_output_dir,\n",
    "        frames_dir=frames_output_dir,  # Set to None to disable frame saving\n",
    "        interval=10\n",
    "    )\n",
    "\n",
    "print(\"\\nAll videos processed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9IxA2DAUOG3h"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMSIFpMFR+aN4pIj49DXKTw",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
