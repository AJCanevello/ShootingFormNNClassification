{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Install & Import"
      ],
      "metadata": {
        "id": "PWybafYl4NOw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mediapipe opencv-python torch torchvision matplotlib scikit-learn"
      ],
      "metadata": {
        "id": "2IYkY90SFY5v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import mediapipe as mp\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms\n",
        "import os\n",
        "from google.colab import drive\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix, classification_report"
      ],
      "metadata": {
        "id": "x58WRh4TFjjv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Com6n71j31oe"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Definitions"
      ],
      "metadata": {
        "id": "lJZ_Wist4d9U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the dataset path\n",
        "data_dir = \"/content/gdrive/MyDrive/E-RAU(DB)/MA680/data/Shooting/Extra Data/op_Processed_Frames\"\n",
        "model_save_path = \"/content/gdrive/MyDrive/E-RAU(DB)/MA680/models/shooting_form_classifier_SFC_1.pth\""
      ],
      "metadata": {
        "id": "WfxJKIcI4hC1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ShootingFormDataset(Dataset):\n",
        "    def __init__(self, data_dir, transform=None):\n",
        "        self.data_dir = data_dir\n",
        "        self.transform = transform\n",
        "        self.images = []\n",
        "        self.labels = []\n",
        "\n",
        "        # Iterate over \"Error Detected\" and \"No Error Detected\" folders\n",
        "        for label, folder in enumerate([\"Error Detected\", \"No Error Detected\"]):\n",
        "            folder_path = os.path.join(data_dir, folder)\n",
        "            for file in os.listdir(folder_path):\n",
        "                if file.endswith(('.jpg', '.png')):\n",
        "                    self.images.append(os.path.join(folder_path, file))\n",
        "                    self.labels.append(label)  # Labels are 0 or 1\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path = self.images[idx]\n",
        "        label = self.labels[idx]\n",
        "        image = cv2.imread(image_path)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, torch.tensor(label, dtype=torch.float32)  # Ensure label is a float tensor"
      ],
      "metadata": {
        "id": "GWjHvIofGYZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data & Preprocessing"
      ],
      "metadata": {
        "id": "FGXm-bVq4hYD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define augmentation for the minority class\n",
        "augmentation = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "])\n",
        "\n",
        "# Upsample the minority class\n",
        "minority_class_path = os.path.join(data_dir, \"No Error Detected\")\n",
        "minority_images = [f for f in os.listdir(minority_class_path) if f.endswith(('.jpg', '.png'))]\n",
        "\n",
        "# Apply augmentation to create more samples\n",
        "upsampled_images = []\n",
        "upsampled_labels = []\n",
        "for image_file in minority_images:\n",
        "    image_path = os.path.join(minority_class_path, image_file)\n",
        "    image = cv2.imread(image_path)\n",
        "    for _ in range(6):  # Upsample by a factor of 6 (360 / 60)\n",
        "        augmented_image = augmentation(image)\n",
        "        upsampled_images.append(augmented_image)\n",
        "        upsampled_labels.append(1)  # Assuming \"No Error Detected\" is label 1"
      ],
      "metadata": {
        "id": "osT3nD1QGfNK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AdvancedShootingFormClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(AdvancedShootingFormClassifier, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool = nn.AdaptiveAvgPool2d((8, 8))\n",
        "        self.fc1 = nn.Linear(128 * 8 * 8, 256)\n",
        "        self.fc2 = nn.Linear(256, 1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "        # Store the activations and gradients\n",
        "        self.gradients = None\n",
        "        self.activations = None\n",
        "\n",
        "    def activations_hook(self, grad):\n",
        "        self.gradients = grad\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.conv1(x))\n",
        "        x = self.relu(self.conv2(x))\n",
        "        x = self.relu(self.conv3(x))\n",
        "\n",
        "        # Register hook to capture gradients\n",
        "        if x.requires_grad:\n",
        "            h = x.register_hook(self.activations_hook)\n",
        "        self.activations = x  # Store activations\n",
        "\n",
        "        x = self.pool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.sigmoid(self.fc2(x))\n",
        "        return x\n",
        "\n",
        "    def get_activations_gradient(self):\n",
        "        return self.gradients\n",
        "\n",
        "    def get_activations(self):\n",
        "        return self.activations"
      ],
      "metadata": {
        "id": "_3i7yRr3Gk7C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Add upsampled images to the dataset\n",
        "dataset = ShootingFormDataset(data_dir, transform=augmentation)\n",
        "dataset.images.extend([image_path] * 6 for image_path in minority_images)\n",
        "dataset.labels.extend(upsampled_labels)\n",
        "\n",
        "# Transforms\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "])\n"
      ],
      "metadata": {
        "id": "FcsMWFCOGnWx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset and DataLoader\n",
        "dataset = ShootingFormDataset(data_dir, transform=transform)\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# Device setup\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Model, loss function, and optimizer\n",
        "model = AdvancedShootingFormClassifier().to(device)\n",
        "criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([6.0]).to(device))  # Use pos_weight for class imbalance\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
      ],
      "metadata": {
        "id": "Rv-MAGD5KHwz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def grad_cam(model, image, target_class=0, threshold=0.5):\n",
        "    \"\"\"\n",
        "    Generate a Grad-CAM heatmap for the given image and target class.\n",
        "    Args:\n",
        "        model: The trained model.\n",
        "        image: Input image tensor.\n",
        "        target_class: The class index for which to generate the heatmap.\n",
        "        threshold: Minimum intensity for the heatmap (default: 0.5).\n",
        "    \"\"\"\n",
        "    # Ensure the input tensor requires gradients\n",
        "    image = image.unsqueeze(0).to(device)\n",
        "    image.requires_grad_(True)\n",
        "\n",
        "    # Forward pass\n",
        "    output = model(image)\n",
        "    output[:, target_class].backward()  # Backpropagate for the target class\n",
        "\n",
        "    # Get the gradients and activations\n",
        "    gradients = model.get_activations_gradient()\n",
        "    activations = model.get_activations()\n",
        "\n",
        "    # Pool the gradients across the channels\n",
        "    pooled_gradients = torch.mean(gradients, dim=[0, 2, 3])\n",
        "\n",
        "    # Weight the activations by the pooled gradients\n",
        "    for i in range(activations.shape[1]):\n",
        "        activations[:, i, :, :] *= pooled_gradients[i]\n",
        "\n",
        "    # Average the weighted activations across the channels\n",
        "    heatmap = torch.mean(activations, dim=1).squeeze().cpu()\n",
        "\n",
        "    # Apply ReLU to the heatmap\n",
        "    heatmap = torch.relu(heatmap)\n",
        "\n",
        "    # Normalize the heatmap to the range [0, 1]\n",
        "    heatmap -= heatmap.min()\n",
        "    heatmap /= heatmap.max()\n",
        "\n",
        "    # Apply a threshold to remove low-intensity regions\n",
        "    heatmap[heatmap < threshold] = 0\n",
        "\n",
        "    return heatmap.detach().numpy()"
      ],
      "metadata": {
        "id": "DB1hzlstDchN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_grad_cam(model, dataloader, num_images=5, threshold=0.5):\n",
        "    \"\"\"\n",
        "    Visualize Grad-CAM heatmaps for a few images.\n",
        "    Args:\n",
        "        model: The trained model.\n",
        "        dataloader: DataLoader for the dataset.\n",
        "        num_images: Number of images to visualize.\n",
        "        threshold: Minimum intensity for the heatmap (default: 0.5).\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    for i, (images, labels) in enumerate(dataloader):\n",
        "        if i >= num_images:  # Limit the number of images to visualize\n",
        "            break\n",
        "\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        # Generate Grad-CAM heatmap\n",
        "        heatmap = grad_cam(model, images[0], target_class=int(labels[0].item()), threshold=threshold)\n",
        "\n",
        "        # Convert image to numpy for visualization\n",
        "        image = images[0].cpu().numpy()\n",
        "        image = np.transpose(image, (1, 2, 0))  # Convert from (C, H, W) to (H, W, C)\n",
        "        image = (image * 0.5) + 0.5  # Undo normalization\n",
        "\n",
        "        # Resize heatmap to match the image size\n",
        "        heatmap = cv2.resize(heatmap, (image.shape[1], image.shape[0]))\n",
        "\n",
        "        # Apply a more vibrant colormap (e.g., JET)\n",
        "        heatmap = np.uint8(255 * heatmap)\n",
        "        heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
        "\n",
        "        # Overlay heatmap on the image with increased opacity\n",
        "        superimposed_img = heatmap * 0.6 + image * 255 * 0.4  # Adjust opacity (0.6 for heatmap, 0.4 for image)\n",
        "        superimposed_img = np.clip(superimposed_img, 0, 255).astype(np.uint8)\n",
        "\n",
        "        # Display the results\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.imshow(image)\n",
        "        plt.title(f\"True: {'No Error Detected' if labels[0].item() == 1 else 'Error Detected'}\")\n",
        "        plt.axis('off')\n",
        "\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.imshow(superimposed_img)\n",
        "        plt.title(\"Grad-CAM Heatmap\")\n",
        "        plt.axis('off')\n",
        "\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "LtdxrZx6Dls4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Training"
      ],
      "metadata": {
        "id": "8IRVCwEx4pki"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Loop\n",
        "epochs = 3\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "\n",
        "    for images, labels in dataloader:\n",
        "        images, labels = images.to(device), labels.to(device).float().unsqueeze(1)  # Add unsqueeze to make labels [batch_size, 1]\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update metrics\n",
        "        running_loss += loss.item()\n",
        "        predictions = (torch.sigmoid(outputs) > 0.5).float()  # Apply sigmoid to get probabilities\n",
        "        correct_predictions += (predictions == labels).sum().item()\n",
        "        total_predictions += labels.size(0)\n",
        "\n",
        "    # Calculate epoch metrics\n",
        "    epoch_loss = running_loss / len(dataloader)\n",
        "    epoch_accuracy = (correct_predictions / total_predictions) * 100\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%\")\n"
      ],
      "metadata": {
        "id": "bj6tVWX-4q1O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model\n",
        "os.makedirs(os.path.dirname(model_save_path), exist_ok=True)\n",
        "torch.save(model.state_dict(), model_save_path)\n",
        "print(f\"Model saved to {model_save_path}\")"
      ],
      "metadata": {
        "id": "5NJzNNoHHQMQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the trained model\n",
        "model = AdvancedShootingFormClassifier().to(device)\n",
        "model.load_state_dict(torch.load(model_save_path, map_location=device))\n",
        "model.eval()\n",
        "\n",
        "# Create a DataLoader for visualization\n",
        "visualization_dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
        "\n",
        "# Visualize Grad-CAM heatmaps with increased intensity\n",
        "visualize_grad_cam(model, visualization_dataloader, num_images=5, threshold=0.5)"
      ],
      "metadata": {
        "id": "Xy1Z9keUHS84"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Application"
      ],
      "metadata": {
        "id": "rQjoNdHF4wRc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define paths\n",
        "data_dir = \"/content/gdrive/MyDrive/E-RAU(DB)/MA680/data/Shooting/op_Processed_Frames\"\n",
        "output_dir = \"/content/gdrive/MyDrive/E-RAU(DB)/MA680/data/Shooting/Evaluated_Frames\"\n",
        "model_save_path = \"/content/gdrive/MyDrive/E-RAU(DB)/MA680/models/shooting_form_classifier_SFC_1.pth\"\n",
        "\n",
        "# Create output directory if it doesn't exist\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Define transforms\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "])\n",
        "\n",
        "# Load the trained model\n",
        "model = AdvancedShootingFormClassifier().to(device)\n",
        "model.load_state_dict(torch.load(model_save_path, map_location=device))\n",
        "model.eval()\n",
        "\n",
        "# Function to evaluate frames\n",
        "def evaluate_frames(model, data_dir, output_dir, transform):\n",
        "    \"\"\"\n",
        "    Evaluate each frame in the directory and save the results.\n",
        "    Args:\n",
        "        model: The trained model.\n",
        "        data_dir: Directory containing the frames.\n",
        "        output_dir: Directory to save the evaluated frames.\n",
        "        transform: Transforms to apply to each frame.\n",
        "    \"\"\"\n",
        "    for folder in [\"Error Detected\", \"No Error Detected\"]:\n",
        "        folder_path = os.path.join(data_dir, folder)\n",
        "        for file_name in os.listdir(folder_path):\n",
        "            if file_name.endswith(('.jpg', '.png')):\n",
        "                # Load the image\n",
        "                image_path = os.path.join(folder_path, file_name)\n",
        "                image = cv2.imread(image_path)\n",
        "                image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "                # Preprocess the image\n",
        "                image_tensor = transform(image_rgb).unsqueeze(0).to(device)\n",
        "\n",
        "                # Get the model's prediction\n",
        "                with torch.no_grad():\n",
        "                    output = model(image_tensor)\n",
        "                    prediction = (torch.sigmoid(output) > 0.5).float().item()\n",
        "\n",
        "                # Add the prediction label to the image\n",
        "                label = \"Error Detected\" if prediction == 1 else \"No Error Detected\"\n",
        "                font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "                cv2.putText(image, label, (10, 30), font, 1, (0, 255, 0) if prediction == 0 else (0, 0, 255), 2)\n",
        "\n",
        "                # Save the evaluated image\n",
        "                output_path = os.path.join(output_dir, file_name)\n",
        "                cv2.imwrite(output_path, image)\n",
        "                print(f\"Processed and saved: {output_path}\")\n",
        "\n",
        "# Evaluate the frames\n",
        "evaluate_frames(model, data_dir, output_dir, transform)"
      ],
      "metadata": {
        "id": "IKLX7--Q4xns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Evaluation"
      ],
      "metadata": {
        "id": "vT8hYxDO4zHN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the trained model\n",
        "model = AdvancedShootingFormClassifier().to(device)\n",
        "model.load_state_dict(torch.load(model_save_path, map_location=device))\n",
        "model.eval()\n",
        "\n",
        "# Create a DataLoader for visualization\n",
        "visualization_dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
        "\n",
        "# Visualize Grad-CAM heatmaps\n",
        "visualize_grad_cam(model, visualization_dataloader, num_images=5)"
      ],
      "metadata": {
        "id": "der2xT0oDyTp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_tiYR6aYRZvd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}