{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8v5AiJTHU693"
      },
      "outputs": [],
      "source": [
        "# Install MediaPipe\n",
        "!pip install mediapipe\n",
        "\n",
        "from google.colab import drive\n",
        "import os\n",
        "import cv2\n",
        "import mediapipe as mp\n",
        "import csv\n",
        "import json\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torchvision.transforms as T\n",
        "from torchvision.models.segmentation import deeplabv3_resnet101\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Check if Google Drive is mounted correctly\n",
        "if not os.path.exists(\"/content/drive/MyDrive\"):\n",
        "    raise Exception(\"Google Drive not mounted. Please mount it using `drive.mount('/content/drive')`.\")\n",
        "\n",
        "# Define video paths\n",
        "video_paths = [\n",
        "#    \"/content/drive/MyDrive/E-RAU(DB)/MA680/data/Shooting/Extra Data/Experienced/Video/Tr1_NIE.MOV\",\n",
        "#    \"/content/drive/MyDrive/E-RAU(DB)/MA680/data/Shooting/Extra Data/Experienced/Video/JoeSupportSide.MOV\",\n",
        "#    \"/content/drive/MyDrive/E-RAU(DB)/MA680/data/Shooting/Extra Data/Experienced/Video/JoeStrongSide.MOV\",\n",
        "#    \"/content/drive/MyDrive/E-RAU(DB)/MA680/data/Shooting/Extra Data/Experienced/Video/Tr2_NIE.MOV\",\n",
        "    \"/content/drive/MyDrive/E-RAU(DB)/MA680/data/Shooting/Extra Data/Novice/Videos/Te1_1.MOV\",\n",
        "    \"/content/drive/MyDrive/E-RAU(DB)/MA680/data/Shooting/Extra Data/Novice/Videos/Te2_2.MOV\",\n",
        "    \"/content/drive/MyDrive/E-RAU(DB)/MA680/data/Shooting/Extra Data/Novice/Videos/Te2_1.MOV\",\n",
        "    \"/content/drive/MyDrive/E-RAU(DB)/MA680/data/Shooting/Extra Data/Novice/Videos/Te2_2.MOV\",\n",
        "    \"/content/drive/MyDrive/E-RAU(DB)/MA680/data/Shooting/Extra Data/Novice/Videos/Te3_1.MOV\",\n",
        "    \"/content/drive/MyDrive/E-RAU(DB)/MA680/data/Shooting/Extra Data/Novice/Videos/Te3_2.MOV\",\n",
        "    \"/content/drive/MyDrive/E-RAU(DB)/MA680/data/Shooting/Extra Data/Novice/Videos/Te4_1.MOV\",\n",
        "    \"/content/drive/MyDrive/E-RAU(DB)/MA680/data/Shooting/Extra Data/Novice/Videos/Te4_2.MOV\",\n",
        "    \"/content/drive/MyDrive/E-RAU(DB)/MA680/data/Shooting/Extra Data/Novice/Videos/Te5_1.MOV\",\n",
        "    \"/content/drive/MyDrive/E-RAU(DB)/MA680/data/Shooting/Extra Data/Novice/Videos/Te5_2.MOV\",\n",
        "    \"/content/drive/MyDrive/E-RAU(DB)/MA680/data/Shooting/Extra Data/Novice/Videos/Te6_1.MOV\",\n",
        "    \"/content/drive/MyDrive/E-RAU(DB)/MA680/data/Shooting/Extra Data/Novice/Videos/Te6_2.MOV\",\n",
        "    \"/content/drive/MyDrive/E-RAU(DB)/MA680/data/Shooting/Extra Data/Novice/Videos/Te7_1.MOV\",\n",
        "    \"/content/drive/MyDrive/E-RAU(DB)/MA680/data/Shooting/Extra Data/Novice/Videos/Te7_2.MOV\",\n",
        "    \"/content/drive/MyDrive/E-RAU(DB)/MA680/data/Shooting/Extra Data/Novice/Videos/Te8_1.MOV\",\n",
        "    \"/content/drive/MyDrive/E-RAU(DB)/MA680/data/Shooting/Extra Data/Novice/Videos/Te8_2.MOV\",\n",
        "    \"/content/drive/MyDrive/E-RAU(DB)/MA680/data/Shooting/Extra Data/Novice/Videos/Te9_1.MOV\",\n",
        "    \"/content/drive/MyDrive/E-RAU(DB)/MA680/data/Shooting/Extra Data/Novice/Videos/Te9_2.MOV\",\n",
        "    \"/content/drive/MyDrive/E-RAU(DB)/MA680/data/Shooting/Extra Data/Novice/Videos/Te10_1.MOV\",\n",
        "    \"/content/drive/MyDrive/E-RAU(DB)/MA680/data/Shooting/Extra Data/Novice/Videos/Te10_2.MOV\",\n",
        "    \"/content/drive/MyDrive/E-RAU(DB)/MA680/data/Shooting/Extra Data/Novice/Videos/Te11_1.MOV\",\n",
        "    \"/content/drive/MyDrive/E-RAU(DB)/MA680/data/Shooting/Extra Data/Novice/Videos/Te11_2.MOV\",\n",
        "#    \"/content/drive/MyDrive/E-RAU(DB)/MA680/data/Shooting/Extra Data/Experienced/Video/Tr3_Anticipating.MOV\",\n",
        "#    \"/content/drive/MyDrive/E-RAU(DB)/MA680/data/Shooting/Extra Data/Experienced/Video/Tr4_TooLittleTriggerFinger.MOV\",\n",
        "#    \"/content/drive/MyDrive/E-RAU(DB)/MA680/data/Shooting/Extra Data/Experienced/Video/Tr5_TooMuchTriggerFinger.MOV\",\n",
        "#    \"/content/drive/MyDrive/E-RAU(DB)/MA680/data/Shooting/Extra Data/Experienced/Video/Tr6_OvergrippingWithPrimaryHand.MOV\",\n",
        "#    \"/content/drive/MyDrive/E-RAU(DB)/MA680/data/Shooting/Extra Data/Experienced/Video/Tr7_OvergrippingWithSecondaryHand.MOV\",\n",
        "#    \"/content/drive/MyDrive/E-RAU(DB)/MA680/data/Shooting/Extra Data/Experienced/Video/Tr8_BreakingWristUp.MOV\",\n",
        "#    \"/content/drive/MyDrive/E-RAU(DB)/MA680/data/Shooting/Extra Data/Experienced/Video/Tr9_CheckingTargetOverSightsBetweenShots.MOV\",\n",
        "#    \"/content/drive/MyDrive/E-RAU(DB)/MA680/data/Shooting/Extra Data/Experienced/Video/Tr10_JerkingTrigger.MOV\",\n",
        "#    \"/content/drive/MyDrive/E-RAU(DB)/MA680/data/Shooting/Extra Data/Experienced/Video/Tr11_LimpWristing.MOV\",\n",
        "#    \"/content/drive/MyDrive/E-RAU(DB)/MA680/data/Shooting/Extra Data/Experienced/Video/Tr13_SightsIncorrectlyAlligned.MOV\",\n",
        "#    \"/content/drive/MyDrive/E-RAU(DB)/MA680/data/Shooting/Extra Data/Experienced/Video/Tr12_GrippingTooLow.MOV\",\n",
        "#    \"/content/drive/MyDrive/E-RAU(DB)/MA680/data/Shooting/Extra Data/Experienced/Video/Tr14_IncorrectHandPlacement.MOV\",\n",
        "\n",
        "    # Add other video paths here\n",
        "]\n",
        "\n",
        "# Check if video files exist\n",
        "for video_path in video_paths:\n",
        "    if not os.path.exists(video_path):\n",
        "        print(f\"Warning: Video file not found - {video_path}\")\n",
        "    else:\n",
        "        print(f\"Video file found: {video_path}\")\n",
        "\n",
        "# Define base output directory\n",
        "base_output_dir = \"/content/drive/MyDrive/E-RAU(DB)/MA680/data/Shooting/Processed_Frames\"\n",
        "os.makedirs(base_output_dir, exist_ok=True)\n",
        "\n",
        "# Initialize MediaPipe solutions\n",
        "mp_pose = mp.solutions.pose\n",
        "mp_hands = mp.solutions.hands\n",
        "mp_face = mp.solutions.face_mesh\n",
        "mp_drawing = mp.solutions.drawing_utils\n",
        "\n",
        "# Load DeepLabV3 model for background removal\n",
        "def load_deeplabv3():\n",
        "    model = deeplabv3_resnet101(pretrained=True, progress=True)\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    return model\n",
        "\n",
        "# Preprocess the frame for DeepLabV3\n",
        "def preprocess_frame(frame):\n",
        "    transform = T.Compose([\n",
        "        T.ToTensor(),  # Convert to tensor\n",
        "        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # Normalize\n",
        "    ])\n",
        "    return transform(frame).unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "# Remove background using DeepLabV3\n",
        "def remove_background(frame, model):\n",
        "    # Preprocess the frame\n",
        "    input_tensor = preprocess_frame(frame)\n",
        "\n",
        "    # Perform inference\n",
        "    with torch.no_grad():\n",
        "        output = model(input_tensor)['out'][0]\n",
        "\n",
        "    # Get the segmentation mask (class 15 is the \"person\" class in DeepLabV3)\n",
        "    mask = output.argmax(0) == 15  # Mask for the \"person\" class\n",
        "\n",
        "    # Convert mask to numpy array\n",
        "    mask = mask.cpu().numpy().astype(np.uint8) * 255\n",
        "\n",
        "    # Apply mask to the original frame\n",
        "    masked_frame = cv2.bitwise_and(frame, frame, mask=mask)\n",
        "\n",
        "    return masked_frame, mask\n",
        "\n",
        "# Function to save keypoints to a CSV file\n",
        "def save_keypoints_to_csv(keypoints, output_path):\n",
        "    with open(output_path, mode='w', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow([\"frame\", \"type\", \"landmark_id\", \"x\", \"y\", \"z\"])\n",
        "        for frame, data in keypoints.items():\n",
        "            for keypoint_type, landmarks in data.items():\n",
        "                for landmark_id, coords in enumerate(landmarks):\n",
        "                    writer.writerow([frame, keypoint_type, landmark_id, coords[0], coords[1], coords[2]])\n",
        "\n",
        "# Function to save keypoints to a JSON file\n",
        "def save_keypoints_to_json(keypoints, output_path):\n",
        "    with open(output_path, 'w') as file:\n",
        "        json.dump(keypoints, file, indent=4)\n",
        "\n",
        "# Function to visualize keypoints and save as an image\n",
        "def visualize_keypoints(keypoints, frame_shape, output_path=None):\n",
        "    \"\"\"\n",
        "    Visualize keypoints on a blank canvas and optionally save the result as an image file.\n",
        "\n",
        "    Args:\n",
        "        keypoints (dict): Dictionary containing keypoints for pose, hands, and face.\n",
        "        frame_shape (tuple): Shape of the original frame (height, width, channels).\n",
        "        output_path (str, optional): Path to save the keypoints visualization. If None, the image is not saved.\n",
        "    \"\"\"\n",
        "    # Create a blank canvas\n",
        "    canvas = np.zeros(frame_shape, dtype=np.uint8)\n",
        "    colors = {\"pose\": (0, 255, 0), \"hands\": (255, 0, 0), \"face\": (0, 0, 255)}\n",
        "\n",
        "    # Draw keypoints on the canvas\n",
        "    for keypoint_type, landmarks in keypoints.items():\n",
        "        if landmarks:\n",
        "            if keypoint_type == \"pose\":\n",
        "                for landmark in landmarks:\n",
        "                    x = int(landmark[0] * frame_shape[1])\n",
        "                    y = int(landmark[1] * frame_shape[0])\n",
        "                    cv2.circle(canvas, (x, y), radius=5, color=colors[keypoint_type], thickness=-1)\n",
        "            elif keypoint_type == \"hands\" or keypoint_type == \"face\":\n",
        "                for landmark_group in landmarks:\n",
        "                    for landmark in landmark_group:\n",
        "                        x = int(landmark[0] * frame_shape[1])\n",
        "                        y = int(landmark[1] * frame_shape[0])\n",
        "                        cv2.circle(canvas, (x, y), radius=5, color=colors[keypoint_type], thickness=-1)\n",
        "\n",
        "    # Save the keypoints visualization if an output path is provided\n",
        "    if output_path:\n",
        "        cv2.imwrite(output_path, canvas)\n",
        "        print(f\"Saved keypoints visualization to: {output_path}\")\n",
        "\n",
        "    # Display the visualization (optional)\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    plt.imshow(cv2.cvtColor(canvas, cv2.COLOR_BGR2RGB))\n",
        "    plt.title(\"Keypoints Visualization\")\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()\n",
        "\n",
        "# Function to process a single frame and extract keypoints\n",
        "def process_frame_and_extract_keypoints(frame, pose, hands, face, frame_count, output_dir, visualize=False):\n",
        "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "    pose_results = pose.process(frame_rgb)\n",
        "    hand_results = hands.process(frame_rgb)\n",
        "    face_results = face.process(frame_rgb)\n",
        "    annotated_frame = frame.copy()\n",
        "    keypoints = {}\n",
        "\n",
        "    if pose_results.pose_landmarks:\n",
        "        mp_drawing.draw_landmarks(annotated_frame, pose_results.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
        "        keypoints[\"pose\"] = [[lm.x, lm.y, lm.z] for lm in pose_results.pose_landmarks.landmark]\n",
        "\n",
        "    if hand_results.multi_hand_landmarks:\n",
        "        keypoints[\"hands\"] = []\n",
        "        for hand_landmarks in hand_results.multi_hand_landmarks:\n",
        "            mp_drawing.draw_landmarks(annotated_frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
        "            keypoints[\"hands\"].append([[lm.x, lm.y, lm.z] for lm in hand_landmarks.landmark])\n",
        "\n",
        "    if face_results.multi_face_landmarks:\n",
        "        keypoints[\"face\"] = []\n",
        "        for face_landmarks in face_results.multi_face_landmarks:\n",
        "            mp_drawing.draw_landmarks(annotated_frame, face_landmarks, mp_face.FACEMESH_CONTOURS)\n",
        "            keypoints[\"face\"].append([[lm.x, lm.y, lm.z] for lm in face_landmarks.landmark])\n",
        "\n",
        "    # Save the keypoints visualization as an image\n",
        "    keypoints_output_path = os.path.join(output_dir, f\"keypoints_frame_{frame_count:04d}.jpg\")\n",
        "    visualize_keypoints(keypoints, frame.shape, output_path=keypoints_output_path)\n",
        "\n",
        "    if visualize:\n",
        "        visualize_keypoints(keypoints, frame.shape)\n",
        "\n",
        "    return annotated_frame, {frame_count: keypoints}\n",
        "\n",
        "# Function to extract frames from a video\n",
        "def extract_and_process_frames(video_path, output_dir, extract_interval=10, visualize=False):\n",
        "    video_name = os.path.splitext(os.path.basename(video_path))[0]\n",
        "    video_output_dir = os.path.join(output_dir, video_name)\n",
        "    os.makedirs(video_output_dir, exist_ok=True)\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    if not cap.isOpened():\n",
        "        print(f\"Error: Could not open video {video_path}.\")\n",
        "        return\n",
        "\n",
        "    # Load DeepLabV3 model\n",
        "    deeplabv3_model = load_deeplabv3()\n",
        "\n",
        "    with mp_pose.Pose(min_detection_confidence=0.28, min_tracking_confidence=0.28) as pose, \\\n",
        "         mp_hands.Hands(min_detection_confidence=0.346109, min_tracking_confidence=0.34609) as hands, \\\n",
        "         mp_face.FaceMesh(min_detection_confidence=0.3, min_tracking_confidence=0.3) as face:\n",
        "        frame_count = 0\n",
        "        extracted_count = 0\n",
        "        all_keypoints = {}\n",
        "        while True:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "            if frame_count % extract_interval == 0:\n",
        "                # Remove background\n",
        "                masked_frame, _ = remove_background(frame, deeplabv3_model)\n",
        "\n",
        "                # Process the frame with MediaPipe and extract keypoints\n",
        "                annotated_frame, keypoints = process_frame_and_extract_keypoints(\n",
        "                    masked_frame, pose, hands, face, frame_count, video_output_dir, visualize\n",
        "                )\n",
        "\n",
        "                # Save the processed frame\n",
        "                output_path = os.path.join(video_output_dir, f\"frame_{frame_count:04d}.jpg\")\n",
        "                cv2.imwrite(output_path, annotated_frame)\n",
        "                print(f\"Processed and saved: {output_path}\")\n",
        "\n",
        "                # Save the keypoints\n",
        "                all_keypoints.update(keypoints)\n",
        "                extracted_count += 1\n",
        "            frame_count += 1\n",
        "        cap.release()\n",
        "        csv_output_path = os.path.join(video_output_dir, \"keypoints.csv\")\n",
        "        save_keypoints_to_csv(all_keypoints, csv_output_path)\n",
        "        print(f\"Saved keypoints to CSV: {csv_output_path}\")\n",
        "        json_output_path = os.path.join(video_output_dir, \"keypoints.json\")\n",
        "        save_keypoints_to_json(all_keypoints, json_output_path)\n",
        "        print(f\"Saved keypoints to JSON: {json_output_path}\")\n",
        "        print(f\"Finished processing {extracted_count} frames from {video_path}\")\n",
        "\n",
        "# Process each video\n",
        "for video_path in video_paths:\n",
        "    extract_and_process_frames(video_path, base_output_dir, visualize=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5PcnbqlNWDju"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}