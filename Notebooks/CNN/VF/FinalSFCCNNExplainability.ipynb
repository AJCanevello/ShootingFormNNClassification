{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "2-Eiigrz_fCw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PUtqt67K_d-U"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, WeightedRandomSampler, random_split, Dataset\n",
        "from torchvision import transforms, datasets, models\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from collections import Counter\n",
        "from PIL import Image\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Step 1: Define the ResNet18 model with Grad-CAM support\n",
        "class ResNetWithGradCAM(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(ResNetWithGradCAM, self).__init__()\n",
        "        self.model = models.resnet18(pretrained=True)\n",
        "        self.num_features = self.model.fc.in_features\n",
        "        self.model.fc = nn.Linear(self.num_features, num_classes)  # Modify the final layer\n",
        "\n",
        "        # Register hooks to capture activations and gradients\n",
        "        self.gradients = None\n",
        "        self.activations = None\n",
        "        self.model.layer4.register_forward_hook(self.activations_hook)\n",
        "        self.model.layer4.register_backward_hook(self.gradients_hook)\n",
        "\n",
        "    def activations_hook(self, module, input, output):\n",
        "        self.activations = output\n",
        "\n",
        "    def gradients_hook(self, module, grad_input, grad_output):\n",
        "        self.gradients = grad_output[0]\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "    def get_activations(self):\n",
        "        return self.activations\n",
        "\n",
        "    def get_gradients(self):\n",
        "        return self.gradients\n",
        "\n",
        "# Step 2: Define the device (CPU or GPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Step 3: Define transformations\n",
        "standard_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "augmentation_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(degrees=10),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Step 4: Custom dataset class\n",
        "class CustomDataset(datasets.ImageFolder):\n",
        "    def __init__(self, root, standard_transform=None, augmentation_transform=None):\n",
        "        super().__init__(root)\n",
        "        self.standard_transform = standard_transform\n",
        "        self.augmentation_transform = augmentation_transform\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        path, label = self.imgs[index]\n",
        "        image = Image.open(path).convert(\"RGB\")\n",
        "\n",
        "        if label == 1 and self.augmentation_transform:\n",
        "            image = self.augmentation_transform(image)\n",
        "        else:\n",
        "            image = self.standard_transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "# Step 5: Define the UnlabeledDataset class\n",
        "class UnlabeledDataset(Dataset):\n",
        "    def __init__(self, root, transform=None):\n",
        "        self.root = root\n",
        "        self.transform = transform\n",
        "        self.image_paths = sorted([os.path.join(root, fname) for fname in os.listdir(root) if fname.endswith(('.jpg', '.png', '.jpeg'))])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path = self.image_paths[idx]\n",
        "        image = Image.open(image_path).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, os.path.basename(image_path)\n",
        "\n",
        "# Step 6: Load the training dataset\n",
        "train_data_dir = \"/content/drive/MyDrive/E-RAU(DB)/MA680/data/Shooting/Processed_Frames/TrainingImages\"\n",
        "train_dataset = CustomDataset(\n",
        "    root=train_data_dir,\n",
        "    standard_transform=standard_transform,\n",
        "    augmentation_transform=augmentation_transform\n",
        ")\n",
        "\n",
        "print(f\"Total dataset size: {len(train_dataset)}\")\n",
        "\n",
        "# Step 7: Calculate class weights\n",
        "class_counts = Counter([label for _, label in train_dataset])\n",
        "total_samples = sum(class_counts.values())\n",
        "class_weights = [total_samples / class_counts[i] for i in range(len(class_counts))]\n",
        "class_weights = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "\n",
        "print(f\"Class weights: {class_weights}\")\n",
        "\n",
        "# Step 8: Calculate sample weights\n",
        "sample_weights = [class_weights[label] for _, label in train_dataset]\n",
        "print(f\"Sample weights length: {len(sample_weights)}\")\n",
        "\n",
        "# Step 9: Split the dataset\n",
        "train_size = int(0.8 * len(train_dataset))\n",
        "val_size = len(train_dataset) - train_size\n",
        "print(f\"Train size: {train_size}, Validation size: {val_size}\")\n",
        "\n",
        "train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
        "\n",
        "# Step 10: Create sampler for training subset\n",
        "train_indices = train_dataset.indices\n",
        "train_sample_weights = [sample_weights[i] for i in train_indices]\n",
        "sampler = WeightedRandomSampler(train_sample_weights, num_samples=len(train_dataset), replacement=True)\n",
        "\n",
        "# Step 11: Create DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, sampler=sampler)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Step 12: Initialize model\n",
        "model = ResNetWithGradCAM(num_classes=2).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
        "\n",
        "# Step 13: Training loop\n",
        "num_epochs = 10\n",
        "best_val_loss = float('inf')\n",
        "patience = 3\n",
        "trigger_times = 0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for images, labels in train_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    scheduler.step()\n",
        "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(train_loader):.4f}\")\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    val_loss /= len(val_loader)\n",
        "    val_accuracy = accuracy_score(all_labels, all_preds)\n",
        "    val_precision = precision_score(all_labels, all_preds, average='binary', pos_label=1)\n",
        "    val_recall = recall_score(all_labels, all_preds, average='binary', pos_label=1)\n",
        "    val_f1 = f1_score(all_labels, all_preds, average='binary', pos_label=1)\n",
        "    print(f\"Validation Loss: {val_loss:.4f}, \"\n",
        "          f\"Accuracy: {val_accuracy:.4f}, Precision: {val_precision:.4f}, \"\n",
        "          f\"Recall: {val_recall:.4f}, F1: {val_f1:.4f}\")\n",
        "\n",
        "    # Early stopping\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        trigger_times = 0\n",
        "        torch.save(model.state_dict(), \"best_pretrained_model.pth\")\n",
        "    else:\n",
        "        trigger_times += 1\n",
        "        if trigger_times >= patience:\n",
        "            print(\"Early stopping!\")\n",
        "            break\n",
        "\n",
        "print(\"Training complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Step 14: Enhanced Dual-Neuron Grad-CAM Implementation\n",
        "def grad_cam_dual_neuron(model, image):\n",
        "    \"\"\"Generate Grad-CAM heatmaps for both classes with proper gradient flow\"\"\"\n",
        "    model.eval()\n",
        "    image = image.unsqueeze(0).to(device)\n",
        "    image.requires_grad = True\n",
        "\n",
        "    # Forward pass\n",
        "    output = model(image)\n",
        "    probs = torch.softmax(output, dim=1)\n",
        "\n",
        "    # Zero gradients\n",
        "    model.zero_grad()\n",
        "\n",
        "    # Create one-hot encodings for both classes\n",
        "    one_hot_pos = torch.zeros_like(output)\n",
        "    one_hot_pos[0, 1] = 1  # Positive class (good form)\n",
        "\n",
        "    one_hot_neg = torch.zeros_like(output)\n",
        "    one_hot_neg[0, 0] = 1  # Negative class (bad form)\n",
        "\n",
        "    # Backpropagate for positive class\n",
        "    output.backward(gradient=one_hot_pos, retain_graph=True)\n",
        "    gradients_pos = model.get_gradients().cpu().detach().numpy()\n",
        "    activations = model.get_activations().cpu().detach().numpy()\n",
        "\n",
        "    # Compute weights for positive class\n",
        "    weights_pos = np.mean(gradients_pos, axis=(2, 3))\n",
        "\n",
        "    # Create heatmap for positive class\n",
        "    heatmap_pos = np.zeros(activations.shape[2:], dtype=np.float32)\n",
        "    for i in range(activations.shape[1]):\n",
        "        heatmap_pos += weights_pos[0, i] * activations[0, i]\n",
        "    heatmap_pos = np.maximum(heatmap_pos, 0)\n",
        "    heatmap_pos = (heatmap_pos - heatmap_pos.min()) / (heatmap_pos.max() - heatmap_pos.min() + 1e-8)\n",
        "\n",
        "    # Backpropagate for negative class\n",
        "    model.zero_grad()\n",
        "    output.backward(gradient=one_hot_neg)\n",
        "    gradients_neg = model.get_gradients().cpu().detach().numpy()\n",
        "\n",
        "    # Compute weights for negative class\n",
        "    weights_neg = np.mean(gradients_neg, axis=(2, 3))\n",
        "\n",
        "    # Create heatmap for negative class\n",
        "    heatmap_neg = np.zeros(activations.shape[2:], dtype=np.float32)\n",
        "    for i in range(activations.shape[1]):\n",
        "        heatmap_neg += weights_neg[0, i] * activations[0, i]\n",
        "    heatmap_neg = np.maximum(heatmap_neg, 0)\n",
        "    heatmap_neg = (heatmap_neg - heatmap_neg.min()) / (heatmap_neg.max() - heatmap_neg.min() + 1e-8)\n",
        "\n",
        "    return heatmap_pos, heatmap_neg, probs[0].cpu().detach().numpy()\n",
        "\n",
        "def visualize_dual_gradcam(image, heatmap_pos, heatmap_neg, pred_prob):\n",
        "    \"\"\"Visualize both class heatmaps side by side with original image\"\"\"\n",
        "    # Convert tensor to numpy if needed\n",
        "    if isinstance(image, torch.Tensor):\n",
        "        image = image.cpu().numpy()\n",
        "        image = np.transpose(image, (1, 2, 0))\n",
        "        # Denormalize\n",
        "        mean = np.array([0.485, 0.456, 0.406])\n",
        "        std = np.array([0.229, 0.224, 0.225])\n",
        "        image = std * image + mean\n",
        "        image = np.clip(image, 0, 1)\n",
        "        image = (image * 255).astype(np.uint8)\n",
        "\n",
        "    # Create figure\n",
        "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 6))\n",
        "\n",
        "    # Show original image\n",
        "    ax1.imshow(image)\n",
        "    ax1.set_title(f\"Original Image\\n(True Class: {'Good' if pred_prob[1] > 0.5 else 'Bad'})\")\n",
        "    ax1.axis('off')\n",
        "\n",
        "    # Prepare heatmaps\n",
        "    heatmap_pos = cv2.resize(heatmap_pos, (image.shape[1], image.shape[0]))\n",
        "    heatmap_neg = cv2.resize(heatmap_neg, (image.shape[1], image.shape[0]))\n",
        "\n",
        "    # Create colored heatmaps\n",
        "    heatmap_pos_viz = cv2.applyColorMap(np.uint8(255 * heatmap_pos), cv2.COLORMAP_JET)\n",
        "    heatmap_neg_viz = cv2.applyColorMap(np.uint8(255 * heatmap_neg), cv2.COLORMAP_JET)\n",
        "\n",
        "    # Create overlays\n",
        "    overlay_pos = cv2.addWeighted(image, 0.5, heatmap_pos_viz, 0.5, 0)\n",
        "    overlay_neg = cv2.addWeighted(image, 0.5, heatmap_neg_viz, 0.5, 0)\n",
        "\n",
        "    # Show positive class heatmap\n",
        "    ax2.imshow(overlay_pos)\n",
        "    ax2.set_title(f\"Good Form Heatmap\\n(Prob: {pred_prob[1]:.2f})\")\n",
        "    ax2.axis('off')\n",
        "\n",
        "    # Show negative class heatmap\n",
        "    ax3.imshow(overlay_neg)\n",
        "    ax3.set_title(f\"Bad Form Heatmap\\n(Prob: {pred_prob[0]:.2f})\")\n",
        "    ax3.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Step 15: Test the Grad-CAM visualization\n",
        "sample_image, sample_label = train_dataset[0]\n",
        "heatmap_pos, heatmap_neg, probs = grad_cam_dual_neuron(model, sample_image)\n",
        "visualize_dual_gradcam(sample_image, heatmap_pos, heatmap_neg, probs)\n",
        "\n",
        "# Step 16: Test dataset evaluation\n",
        "test_data_dir = \"/content/drive/MyDrive/E-RAU(DB)/MA680/data/Shooting/Processed_Frames/X.Test/Frames\"\n",
        "test_dataset = UnlabeledDataset(root=test_data_dir, transform=standard_transform)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Load best model\n",
        "model.load_state_dict(torch.load(\"best_pretrained_model.pth\"))\n",
        "model.eval()\n",
        "\n",
        "# Initialize lists\n",
        "all_preds = []\n",
        "all_filenames = []\n",
        "\n",
        "# Run predictions\n",
        "with torch.no_grad():\n",
        "    for images, filenames in test_loader:\n",
        "        images = images.to(device)\n",
        "        outputs = model(images)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_filenames.extend(filenames)\n",
        "\n",
        "# Save predictions\n",
        "results = pd.DataFrame({\"Filename\": all_filenames, \"Prediction\": all_preds})\n",
        "results.to_csv(\"test_predictions.csv\", index=False)\n",
        "print(\"Test predictions saved to test_predictions.csv\")\n",
        "\n",
        "# Step 17: Visualize Grad-CAM for test images\n",
        "num_samples = 100  # Number of test images to visualize\n",
        "for i in range(min(num_samples, len(test_dataset))):\n",
        "    sample_image, sample_filename = test_dataset[i]\n",
        "    heatmap_pos, heatmap_neg, probs = grad_cam_dual_neuron(model, sample_image)\n",
        "    visualize_dual_gradcam(sample_image, heatmap_pos, heatmap_neg, probs)\n",
        "    print(f\"Filename: {sample_filename}, Prediction: {'Good' if all_preds[i] == 1 else 'Bad'}\")"
      ],
      "metadata": {
        "id": "RjBOIV4t_hWc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0qtxoKozAuTp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}