{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Npa-T7ThsXPj"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms\n",
        "from torchvision import transforms, datasets\n",
        "from torchvision.datasets import ImageFolder\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Define the CNN model\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
        "        self.fc1 = nn.Linear(32 * 8 * 8, 128)  # Adjust input size based on your image dimensions\n",
        "        self.fc2 = nn.Linear(128, num_classes)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(self.relu(self.conv1(x)))\n",
        "        x = self.pool(self.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 32 * 8 * 8)  # Flatten the tensor\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "gSfuIJuVtnWI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Define the device (CPU or GPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n"
      ],
      "metadata": {
        "id": "rB8H6j20tpEv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Define class weights (adjust based on your dataset)\n",
        "# Example: If class 0 has fewer samples than class 1, assign a higher weight to class 0\n",
        "class_weights = torch.tensor([2.0, 1.0])  # [weight for class 0, weight for class 1]\n",
        "class_weights = class_weights.to(device)"
      ],
      "metadata": {
        "id": "FVpTWcLFtrBA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Define the loss function with class weights\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights)"
      ],
      "metadata": {
        "id": "3YVQ9IE-ts-X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Define the optimizer\n",
        "model = CNN(num_classes=2).to(device)  # 2 classes: 0.NoError and 1.Error\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "uaPE4_TctkfO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Load the dataset\n",
        "# Define the path to your dataset\n",
        "data_dir = \"/content/drive/MyDrive/E-RAU(DB)/MA680/data/Shooting/TrainingData\"\n",
        "\n",
        "# Define transformations for the training data\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((32, 32)),  # Resize images to 32x32\n",
        "    transforms.ToTensor(),         # Convert images to PyTorch tensors\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize images\n",
        "])\n",
        "\n",
        "# Load the dataset using ImageFolder\n",
        "train_dataset = datasets.ImageFolder(root=data_dir, transform=transform)\n",
        "\n",
        "# Create a DataLoader\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)"
      ],
      "metadata": {
        "id": "QJuMYe_ctyVb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: Training loop\n",
        "num_epochs = 3\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for images, labels in train_loader:\n",
        "        # Move data to the appropriate device\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(train_loader):.4f}\")\n",
        "\n",
        "print(\"Training complete!\")"
      ],
      "metadata": {
        "id": "Y5hpbDGrHeeh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the trained model\n",
        "torch.save(model.state_dict(), \"trained_CNNmodel.pth\")\n",
        "print(\"Model saved.\")"
      ],
      "metadata": {
        "id": "AXjTs-qRVZxs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Analysis"
      ],
      "metadata": {
        "id": "QTrQU-_Od00s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Define a test dataset\n",
        "class TestDataset(Dataset):\n",
        "    def __init__(self, test_dir, transform=None):\n",
        "        self.image_paths = [os.path.join(test_dir, fname) for fname in os.listdir(test_dir) if fname.endswith(\".jpg\")]\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path = self.image_paths[idx]\n",
        "        image = Image.open(image_path).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, image_path  # Return image path for visualization"
      ],
      "metadata": {
        "id": "UtnI2mo8Q2S7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Load test data\n",
        "test_dir = \"/content/drive/MyDrive/E-RAU(DB)/MA680/data/Shooting/Processed_Frames/X.Test/Images\"\n",
        "test_dataset = TestDataset(test_dir=test_dir, transform=transform)\n",
        "\n",
        "# Check if test dataset is empty\n",
        "if len(test_dataset) == 0:\n",
        "    print(\"Error: No .jpg files found in the test directory.\")\n",
        "else:\n",
        "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "9gcjEgOyQ3fe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    # Step 3: Evaluate the model on the test set\n",
        "    model.eval()\n",
        "    test_predictions = []\n",
        "    test_image_paths = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, paths in test_loader:\n",
        "            images = images.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            test_predictions.extend(predicted.cpu().numpy())\n",
        "            test_image_paths.extend(paths)"
      ],
      "metadata": {
        "id": "0lQisvrvd83n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    # Step 4: Analyze predictions\n",
        "    for image_path, prediction in zip(test_image_paths, test_predictions):\n",
        "        print(f\"Image: {image_path}, Predicted: {'Error' if prediction == 1 else 'NoError'}\")"
      ],
      "metadata": {
        "id": "OgmNrj6EeBe2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    # Step 5: Visualize a few test predictions\n",
        "    num_samples = min(5, len(test_image_paths))  # Ensure we don't exceed the number of available samples\n",
        "    for i in range(num_samples):\n",
        "        image_path = test_image_paths[i]\n",
        "        prediction = test_predictions[i]\n",
        "\n",
        "        image = Image.open(image_path).convert(\"RGB\")\n",
        "        plt.imshow(image)\n",
        "        plt.title(f\"Predicted: {'Error' if prediction == 1 else 'NoError'}\")\n",
        "        plt.axis(\"off\")\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "Tsb6j0I1XDBl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Applying GradCam"
      ],
      "metadata": {
        "id": "IuM_kofVciW-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms, datasets\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "# Step 1: Define the CNN model\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        # Adjust the input size for fc1 to match the saved model\n",
        "        self.fc1 = nn.Linear(32 * 8 * 8, 128)  # Input size: 32 * 8 * 8 = 2048\n",
        "        self.fc2 = nn.Linear(128, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(x.size(0), -1)  # Flatten the tensor while preserving the batch dimension\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Step 2: Load the trained model\n",
        "model = CNN(num_classes=2)  # 2 classes: 0.NoError and 1.Error\n",
        "model.load_state_dict(torch.load(\"trained_CNNmodel.pth\", weights_only=True))  # Load your trained model\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "# Step 3: Define the Grad-CAM function\n",
        "class GradCAM:\n",
        "    def __init__(self, model, target_layer):\n",
        "        self.model = model\n",
        "        self.target_layer = target_layer\n",
        "        self.gradients = None\n",
        "        self.activations = None\n",
        "\n",
        "        # Hook into the target layer\n",
        "        target_layer.register_forward_hook(self.save_activations)\n",
        "        target_layer.register_backward_hook(self.save_gradients)\n",
        "\n",
        "    def save_activations(self, module, input, output):\n",
        "        self.activations = output\n",
        "\n",
        "    def save_gradients(self, module, grad_input, grad_output):\n",
        "        self.gradients = grad_output[0]\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "    def backward(self, outputs):\n",
        "        self.model.zero_grad()\n",
        "        outputs.backward(torch.ones_like(outputs))\n",
        "\n",
        "    def generate(self, x, class_idx=None):\n",
        "        # Forward pass\n",
        "        outputs = self.forward(x)\n",
        "        print(f\"Model outputs shape: {outputs.shape}\")  # Debug statement\n",
        "        if class_idx is None:\n",
        "            class_idx = torch.argmax(outputs, dim=1).item()\n",
        "\n",
        "        # Backward pass\n",
        "        self.backward(outputs[:, class_idx].sum())\n",
        "\n",
        "        # Compute Grad-CAM\n",
        "        weights = torch.mean(self.gradients, dim=[2, 3], keepdim=True)\n",
        "        cam = torch.sum(weights * self.activations, dim=1, keepdim=True)\n",
        "        cam = F.relu(cam)  # Apply ReLU to highlight positive influences\n",
        "        cam = F.interpolate(cam, size=(x.shape[2], x.shape[3]), mode=\"bilinear\", align_corners=False)\n",
        "        cam = cam - cam.min()\n",
        "        cam = cam / cam.max()\n",
        "        return cam.squeeze().cpu().detach().numpy()\n",
        "\n",
        "# Step 4: Load and preprocess an example image\n",
        "def preprocess_image(image_path, target_size=(32, 32)):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize(target_size),  # Resize to match model input size\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "    ])\n",
        "    image = datasets.folder.default_loader(image_path)\n",
        "    image = transform(image).unsqueeze(0)  # Add batch dimension\n",
        "    return image\n",
        "\n",
        "# Step 5: Apply Grad-CAM to an image\n",
        "def apply_gradcam(image_path, model, target_layer, target_size=(32, 32)):\n",
        "    # Preprocess the image\n",
        "    image = preprocess_image(image_path, target_size)\n",
        "    image = image.to(device)\n",
        "\n",
        "    # Initialize Grad-CAM\n",
        "    grad_cam = GradCAM(model, target_layer)\n",
        "\n",
        "    # Generate the heatmap\n",
        "    heatmap = grad_cam.generate(image)\n",
        "\n",
        "    # Load the original image for visualization\n",
        "    original_image = cv2.imread(image_path)\n",
        "    original_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Resize the heatmap to match the original image resolution\n",
        "    heatmap = cv2.resize(heatmap, (original_image.shape[1], original_image.shape[0]))\n",
        "\n",
        "    # Normalize the heatmap\n",
        "    heatmap = np.uint8(255 * heatmap)\n",
        "    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
        "\n",
        "    # Overlay the heatmap on the original image\n",
        "    superimposed_img = heatmap * 0.4 + original_image * 0.6\n",
        "    superimposed_img = np.uint8(superimposed_img)\n",
        "\n",
        "    # Display the results\n",
        "    plt.figure(figsize=(15, 10))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.title(\"Original Image\")\n",
        "    plt.imshow(original_image)\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.title(\"Grad-CAM Heatmap\")\n",
        "    plt.imshow(superimposed_img)\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Step 6: Define the target layer for Grad-CAM\n",
        "# Choose the last convolutional layer in your model\n",
        "target_layer = model.conv2\n",
        "\n",
        "# Step 7: Apply Grad-CAM to an example image\n",
        "image_path = \"/content/drive/MyDrive/E-RAU(DB)/MA680/data/Shooting/Processed_Frames/With Background/GoodFormWithBackground/Tr1_NIE/frame_0300.jpg\"  # Replace with the path to your high-resolution image\n",
        "apply_gradcam(image_path, model, target_layer, target_size=(32, 32))  # Use the correct input size (e.g., 32x32)"
      ],
      "metadata": {
        "id": "giBsb7mhU4sm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-nB3UBDrWBPO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}